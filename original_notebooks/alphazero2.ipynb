{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2qCs_HQos_3",
        "outputId": "3d3bced3-2b31-4783-ec3f-1c6876ea2ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/231.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/231.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6kgGzCYoild",
        "outputId": "e6d22525-3deb-4870-864c-07ba28edb80c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating new category\n",
            "\n",
            "=== Iteration 1/2 ===\n",
            "Chosen Parent Id 1\n",
            "creating new category\n",
            "Inspirations Length Parent Insps:2, Child Insps: 2\n",
            "\n",
            "Parent Inspiration:\n",
            " The scoring function appears to be directly the numerical value of the 'result' variable; update the prompt to instruct the model to explicitly maximize this final numerical value.\n",
            "\n",
            "Parent Inspiration:\n",
            " The Child's added complexity (parameters, docstrings) did not improve the score and might be unnecessary; prompt should clarify that simpler code is preferred unless complexity directly leads to a higher 'result' value.\n",
            "\n",
            "Child Inspiration:\n",
            " The score likely rewards a more complex 'meaningful calculation' than simple addition. I will update the prompt to suggest a function that performs a multi-step operation or uses a more specialized mathematical function to increase its perceived utility.\n",
            "\n",
            "Child Inspiration:\n",
            " The scoring might favor a function that demonstrates greater versatility in handling input types. I will update the prompt to encourage the use of non-numerical parameters (like strings or lists) or a variable number of arguments, making the `compute` function more broadly applicable.\n",
            "Iteration 1 took 30.34 seconds\n",
            "[{'id': 1, 'parent_result_id': 1, 'description': '', 'result_id': 2}, {'id': 2, 'parent_result_id': 1, 'description': \"The scoring function appears to be directly the numerical value of the 'result' variable; update the prompt to instruct the model to explicitly maximize this final numerical value.\", 'result_id': None}, {'id': 3, 'parent_result_id': 1, 'description': \"The Child's added complexity (parameters, docstrings) did not improve the score and might be unnecessary; prompt should clarify that simpler code is preferred unless complexity directly leads to a higher 'result' value.\", 'result_id': None}, {'id': 4, 'parent_result_id': 2, 'description': \"The score likely rewards a more complex 'meaningful calculation' than simple addition. I will update the prompt to suggest a function that performs a multi-step operation or uses a more specialized mathematical function to increase its perceived utility.\", 'result_id': None}, {'id': 5, 'parent_result_id': 2, 'description': 'The scoring might favor a function that demonstrates greater versatility in handling input types. I will update the prompt to encourage the use of non-numerical parameters (like strings or lists) or a variable number of arguments, making the `compute` function more broadly applicable.', 'result_id': None}]\n",
            "\n",
            "=== Categories ===\n",
            "\n",
            "Category cat-1:\n",
            "  - Result 1: score=10\n",
            "\n",
            "Category cat-2:\n",
            "  - Result 2: score=8\n",
            "\n",
            "=== Iteration 2/2 ===\n",
            "Chosen Parent Id 1\n",
            "creating new category\n",
            "Inspirations Length Parent Insps:3, Child Insps: 3\n",
            "\n",
            "Parent Inspiration:\n",
            " The score is the raw numerical value of 'result', suggesting potential for very large numbers. Update the prompt to encourage generating the largest possible integer or floating-point number for the 'result' variable.\n",
            "\n",
            "Parent Inspiration:\n",
            " The evaluation implicitly targets a variable named 'result'. Explicitly state in the prompt that the final computed value must be assigned to a variable named 'result' to ensure it is correctly evaluated.\n",
            "\n",
            "Parent Inspiration:\n",
            " If maximizing the 'result' value conflicts with keeping the code 'short and simple', prioritize the score. Clarify that the primary objective is to achieve the highest possible numerical value for 'result', even if it requires slightly more complex calculations.\n",
            "\n",
            "Child Inspiration:\n",
            " The score is directly the value of 'result'. To maximize it, instruct the model to use larger base numbers or operations that inherently scale to higher values, like exponentiation, while maintaining simplicity.\n",
            "\n",
            "Child Inspiration:\n",
            " The current code uses simple multiplication. Prompt the model to explore other arithmetic operations or combinations of operations that could yield significantly higher 'result' values concisely.\n",
            "\n",
            "Child Inspiration:\n",
            " The program uses a function for calculation. Clarify that the 'result' variable should be assigned its maximized value as directly and concisely as possible, without unnecessary function definitions unless they contribute to a higher score or clearer structure.\n",
            "creating new category\n",
            "Inspirations Length Parent Insps:3, Child Insps: 3\n",
            "\n",
            "Parent Inspiration:\n",
            " The scoring function likely places high emphasis on the sheer magnitude of the numerical 'result' value; prompt should instruct the model to generate an extremely large number, explicitly exploring the maximum possible value within Python's limits.\n",
            "\n",
            "Parent Inspiration:\n",
            " The scoring function may implicitly reward extreme minimalism; prompt should suggest directly assigning a large numerical value to 'result' without unnecessary function definitions, testing if this simpler structure is preferred for score maximization.\n",
            "\n",
            "Parent Inspiration:\n",
            " To maximize the score, the prompt should encourage the use of Python's built-in mathematical operations (like '**' for exponentiation) to programmatically compute and assign an extremely large numerical value to 'result', rather than hardcoding a long literal number, while maintaining conciseness.\n",
            "\n",
            "Child Inspiration:\n",
            " The current generated code produces a very small numerical result. Update the prompt to explicitly instruct the model to generate a significantly larger numerical value for the 'result' variable, suggesting target magnitudes like thousands or millions, while still maintaining code simplicity.\n",
            "\n",
            "Child Inspiration:\n",
            " Clarify that there are no implied upper limits or practical constraints on the magnitude of the 'result' variable, encouraging the model to produce the largest possible number using simple and correct operations.\n",
            "\n",
            "Child Inspiration:\n",
            " The instruction to 'prioritize simplicity' might be misinterpreted as a constraint on the numerical value of 'result'. Clarify that simplicity applies to the code structure and not to the magnitude of the number to be maximized.\n",
            "Iteration 2 took 33.63 seconds\n",
            "[{'id': 1, 'parent_result_id': 1, 'description': '', 'result_id': 2}, {'id': 2, 'parent_result_id': 1, 'description': \"The scoring function appears to be directly the numerical value of the 'result' variable; update the prompt to instruct the model to explicitly maximize this final numerical value.\", 'result_id': 3}, {'id': 3, 'parent_result_id': 1, 'description': \"The Child's added complexity (parameters, docstrings) did not improve the score and might be unnecessary; prompt should clarify that simpler code is preferred unless complexity directly leads to a higher 'result' value.\", 'result_id': 4}, {'id': 4, 'parent_result_id': 2, 'description': \"The score likely rewards a more complex 'meaningful calculation' than simple addition. I will update the prompt to suggest a function that performs a multi-step operation or uses a more specialized mathematical function to increase its perceived utility.\", 'result_id': None}, {'id': 5, 'parent_result_id': 2, 'description': 'The scoring might favor a function that demonstrates greater versatility in handling input types. I will update the prompt to encourage the use of non-numerical parameters (like strings or lists) or a variable number of arguments, making the `compute` function more broadly applicable.', 'result_id': None}, {'id': 6, 'parent_result_id': 1, 'description': \"The score is the raw numerical value of 'result', suggesting potential for very large numbers. Update the prompt to encourage generating the largest possible integer or floating-point number for the 'result' variable.\", 'result_id': None}, {'id': 7, 'parent_result_id': 1, 'description': \"The evaluation implicitly targets a variable named 'result'. Explicitly state in the prompt that the final computed value must be assigned to a variable named 'result' to ensure it is correctly evaluated.\", 'result_id': None}, {'id': 8, 'parent_result_id': 1, 'description': \"If maximizing the 'result' value conflicts with keeping the code 'short and simple', prioritize the score. Clarify that the primary objective is to achieve the highest possible numerical value for 'result', even if it requires slightly more complex calculations.\", 'result_id': None}, {'id': 9, 'parent_result_id': 3, 'description': \"The score is directly the value of 'result'. To maximize it, instruct the model to use larger base numbers or operations that inherently scale to higher values, like exponentiation, while maintaining simplicity.\", 'result_id': None}, {'id': 10, 'parent_result_id': 3, 'description': \"The current code uses simple multiplication. Prompt the model to explore other arithmetic operations or combinations of operations that could yield significantly higher 'result' values concisely.\", 'result_id': None}, {'id': 11, 'parent_result_id': 3, 'description': \"The program uses a function for calculation. Clarify that the 'result' variable should be assigned its maximized value as directly and concisely as possible, without unnecessary function definitions unless they contribute to a higher score or clearer structure.\", 'result_id': None}, {'id': 12, 'parent_result_id': 1, 'description': \"The scoring function likely places high emphasis on the sheer magnitude of the numerical 'result' value; prompt should instruct the model to generate an extremely large number, explicitly exploring the maximum possible value within Python's limits.\", 'result_id': None}, {'id': 13, 'parent_result_id': 1, 'description': \"The scoring function may implicitly reward extreme minimalism; prompt should suggest directly assigning a large numerical value to 'result' without unnecessary function definitions, testing if this simpler structure is preferred for score maximization.\", 'result_id': None}, {'id': 14, 'parent_result_id': 1, 'description': \"To maximize the score, the prompt should encourage the use of Python's built-in mathematical operations (like '**' for exponentiation) to programmatically compute and assign an extremely large numerical value to 'result', rather than hardcoding a long literal number, while maintaining conciseness.\", 'result_id': None}, {'id': 15, 'parent_result_id': 4, 'description': \"The current generated code produces a very small numerical result. Update the prompt to explicitly instruct the model to generate a significantly larger numerical value for the 'result' variable, suggesting target magnitudes like thousands or millions, while still maintaining code simplicity.\", 'result_id': None}, {'id': 16, 'parent_result_id': 4, 'description': \"Clarify that there are no implied upper limits or practical constraints on the magnitude of the 'result' variable, encouraging the model to produce the largest possible number using simple and correct operations.\", 'result_id': None}, {'id': 17, 'parent_result_id': 4, 'description': \"The instruction to 'prioritize simplicity' might be misinterpreted as a constraint on the numerical value of 'result'. Clarify that simplicity applies to the code structure and not to the magnitude of the number to be maximized.\", 'result_id': None}]\n",
            "\n",
            "=== Categories ===\n",
            "\n",
            "Category cat-1:\n",
            "  - Result 1: score=10\n",
            "\n",
            "Category cat-2:\n",
            "  - Result 2: score=8\n",
            "\n",
            "Category cat-3:\n",
            "  - Result 3: score=20\n",
            "\n",
            "Category cat-4:\n",
            "  - Result 4: score=20\n",
            "Final Categories: \n",
            "\n",
            "\n",
            "=== Categories ===\n",
            "\n",
            "Category cat-1:\n",
            "  - Result 1: score=10\n",
            "\n",
            "Category cat-2:\n",
            "  - Result 2: score=8\n",
            "\n",
            "Category cat-3:\n",
            "  - Result 3: score=20\n",
            "\n",
            "Category cat-4:\n",
            "  - Result 4: score=20\n"
          ]
        }
      ],
      "source": [
        "from typing import Any, List, Tuple, Callable\n",
        "import sys\n",
        "sys.set_int_max_str_digits(0)\n",
        "import random\n",
        "import numpy as np\n",
        "from google import genai\n",
        "import asyncio\n",
        "import time\n",
        "\n",
        "# ------------------- Database -------------------\n",
        "\n",
        "class Database:\n",
        "    def __init__(self, num_categories = 5, mutation_rate = 0.1, num_inspirations = 5, num_elites = 5):\n",
        "        self.results = []  # main generated programs\n",
        "        self.inspirations = []  # new ideas to try\n",
        "        self._next_result_id = 1\n",
        "        self._next_inspiration_id = 1\n",
        "        self.client = genai.Client()\n",
        "        self.num_categories = num_categories\n",
        "        self.mutation_rate = mutation_rate\n",
        "        self.num_inspirations = num_inspirations\n",
        "        self.num_elites = num_elites\n",
        "\n",
        "    def print_categories(self):\n",
        "        \"\"\"Print all categories and their associated results (ids + scores).\"\"\"\n",
        "        print(\"\\n=== Categories ===\")\n",
        "\n",
        "        # Group results by category\n",
        "        categories = {}\n",
        "        for entry in self.results:\n",
        "            cat = entry.get(\"category\", None)\n",
        "            if cat is not None:\n",
        "                categories.setdefault(cat, []).append(entry)\n",
        "\n",
        "        # Print grouped results\n",
        "        for cat, entries in categories.items():\n",
        "            print(f\"\\nCategory {cat}:\")\n",
        "            for e in entries:\n",
        "                score = e[\"results\"].get(\"score\", 0)\n",
        "                print(f\"  - Result {e['id']}: score={score}\")\n",
        "\n",
        "    def cosine_similarity(self, a, b):\n",
        "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "    def add_result(self, program, results, base_prompt, embedding):\n",
        "        # Decide whether to create a new category\n",
        "        if len(self.results) < self.num_categories:\n",
        "            # make a new category\n",
        "            category = f\"cat-{len(self.results) + 1}\"\n",
        "            print(\"creating new category\")\n",
        "        else:\n",
        "            # Assign category by embedding similarity\n",
        "            sims = [\n",
        "                (self.cosine_similarity(embedding, r[\"embedding\"]), r)\n",
        "                for r in self.results\n",
        "            ]\n",
        "            best_sim, closest_result = max(sims, key=lambda x: x[0])\n",
        "            category = closest_result[\"category\"]\n",
        "\n",
        "        result_entry = {\n",
        "            \"id\": self._next_result_id,\n",
        "            \"program\": program,\n",
        "            \"results\": results,\n",
        "            \"base_prompt\": base_prompt,\n",
        "            \"embedding\": embedding,\n",
        "            \"category\": category,\n",
        "        }\n",
        "\n",
        "        self.results.append(result_entry)\n",
        "\n",
        "        self._next_result_id += 1\n",
        "        return result_entry\n",
        "\n",
        "    def add_inspiration(self, parent_result_id, description, result_id=None):\n",
        "        inspiration_entry = {\n",
        "            \"id\": self._next_inspiration_id,\n",
        "            \"parent_result_id\": parent_result_id,\n",
        "            \"description\": description,\n",
        "            \"result_id\": result_id\n",
        "        }\n",
        "        self.inspirations.append(inspiration_entry)\n",
        "        self._next_inspiration_id += 1\n",
        "        return inspiration_entry[\"id\"]\n",
        "\n",
        "    def sample(self):\n",
        "      if not self.results:\n",
        "          return None\n",
        "\n",
        "      # pick a random category\n",
        "      categories = list(set(r[\"category\"] for r in self.results))\n",
        "      category = random.choice(categories)\n",
        "\n",
        "      # filter results from this category\n",
        "      cat_results = [r for r in self.results if r[\"category\"] == category]\n",
        "\n",
        "      # pick top 5 in that category by score\n",
        "      top_cat_results = sorted(cat_results, key=lambda r: r[\"results\"].get(\"score\", 0), reverse=True)[:self.num_elites]\n",
        "\n",
        "      selected_entry = random.choice(top_cat_results)\n",
        "\n",
        "      # Get all inspirations for this parent that don't yet have a generated result\n",
        "      all_inspirations = [\n",
        "          insp\n",
        "          for insp in self.inspirations\n",
        "          if insp[\"parent_result_id\"] == selected_entry[\"id\"]\n",
        "      ]\n",
        "\n",
        "      unused_inspirations = [\n",
        "          insp for insp in all_inspirations if insp[\"result_id\"] is None\n",
        "      ][:self.num_inspirations]\n",
        "\n",
        "      # === NEW: Cross-category inspiration swap (mutation) ===\n",
        "      if random.random() < self.mutation_rate and len(categories) > 1:\n",
        "          # pick a different category\n",
        "          other_category = random.choice([c for c in categories if c != category])\n",
        "\n",
        "          # filter results from that other category\n",
        "          other_results = [r for r in self.results if r[\"category\"] == other_category]\n",
        "\n",
        "          # pick top 5 in the other category\n",
        "          top_other_results = sorted(other_results, key=lambda r: r[\"results\"].get(\"score\", 0), reverse=True)[:self.num_elites]\n",
        "\n",
        "          other_selected = random.choice(top_other_results)\n",
        "\n",
        "          # swap inspirations (but keep the original selected_entry!)\n",
        "          all_inspirations = [\n",
        "              insp for insp in self.inspirations if insp[\"parent_result_id\"] == other_selected[\"id\"]\n",
        "          ]\n",
        "          unused_inspirations = [\n",
        "              insp for insp in all_inspirations if insp[\"result_id\"] is None\n",
        "          ][:self.num_inspirations]\n",
        "\n",
        "          print(f\"[Mutation] Swapped inspirations from category {category} → {other_category}\")\n",
        "\n",
        "      # sample one of them at random\n",
        "      return selected_entry, unused_inspirations, all_inspirations\n",
        "\n",
        "\n",
        "    def best(self):\n",
        "        if not self.results:\n",
        "            return None\n",
        "        return max(self.results, key=lambda r: (r[\"results\"].get(\"score\", 0), r[\"id\"]))\n",
        "\n",
        "    def mark_inspiration_as_used(self, inspiration: Any, result_id: int):\n",
        "        \"\"\"\n",
        "        Updates the inspiration that matches `inspiration_description` and has no result_id yet,\n",
        "        setting its `result_id` to the given result_id.\n",
        "        \"\"\"\n",
        "        for insp_entry in self.inspirations:\n",
        "            if insp_entry[\"id\"] == inspiration[\"id\"] and insp_entry[\"result_id\"] is None:\n",
        "                insp_entry[\"result_id\"] = result_id\n",
        "\n",
        "# ------------------- Prompt Sampler -------------------\n",
        "\n",
        "class PromptSampler:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm  # LLM instance used to generate new base prompt\n",
        "\n",
        "    async def build(self, parent_program: str, base_prompt: str, inspiration: str) -> str:\n",
        "        \"\"\"\n",
        "        Use the LLM to generate a new base prompt given the previous base prompt,\n",
        "        the parent program, and an inspiration. Returns the combined prompt for diff generation.\n",
        "        \"\"\"\n",
        "        if inspiration:\n",
        "            new_base_prompt = await self.llm.generate_new_base_prompt(\n",
        "                base_prompt=base_prompt,\n",
        "                parent_program=parent_program,\n",
        "                inspiration=inspiration\n",
        "            )\n",
        "        else:\n",
        "            new_base_prompt = base_prompt\n",
        "\n",
        "        # Combine into full prompt for diff generation\n",
        "        combined_prompt = f\"\"\"\n",
        "Base Prompt for this iteration:\n",
        "{new_base_prompt}\n",
        "\n",
        "Parent Program:\n",
        "{parent_program}\n",
        "\n",
        "Instructions:\n",
        "Generate diffs to improve the parent program. Use the following format for all changes:\n",
        "\n",
        "<<<<<<< SEARCH\n",
        "# Original code block to be found and replaced\n",
        "=======\n",
        "# New code block to replace the original\n",
        ">>>>>>> REPLACE\n",
        "\n",
        "Make sure the final computed value to be evaluated is assigned to the variable `result`.\n",
        "\"\"\"\n",
        "        return combined_prompt, new_base_prompt\n",
        "\n",
        "# ------------------- LLM -------------------\n",
        "\n",
        "from pydantic import BaseModel\n",
        "class Inspiration(BaseModel):\n",
        "    description: str\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self):\n",
        "        self.client = genai.Client() # Gets API key from GEMINI_API_KEY\n",
        "\n",
        "    async def embed_program(self, program: str):\n",
        "        \"\"\"Generate embedding vector for a program string.\"\"\"\n",
        "        response = await self.client.aio.models.embed_content(\n",
        "            model=\"gemini-embedding-001\",\n",
        "            contents=program,\n",
        "        )\n",
        "        return np.array(response.embeddings[0].values)\n",
        "\n",
        "    async def generate(self, base_prompt: str):\n",
        "        response = await self.client.aio.models.generate_content(\n",
        "            model=\"gemini-2.0-flash-lite\",\n",
        "            contents=base_prompt\n",
        "        )\n",
        "        diff_text = response.candidates[0].content.parts[0].text\n",
        "        return diff_text, base_prompt\n",
        "\n",
        "    def apply_diff(self, parent_program: str, diff: str):\n",
        "        import re\n",
        "        pattern = re.compile(\n",
        "            r\"<<<<<<< SEARCH\\n(.*?)\\n=======\\n(.*?)\\n>>>>>>> REPLACE\",\n",
        "            re.DOTALL\n",
        "        )\n",
        "\n",
        "        updated_program = parent_program\n",
        "        for match in pattern.finditer(diff):\n",
        "            original_block = match.group(1).strip(\"\\n\")\n",
        "            new_block = match.group(2).strip(\"\\n\")\n",
        "            updated_program = updated_program.replace(original_block, new_block)\n",
        "        return updated_program\n",
        "\n",
        "    async def generate_inspiration_regression(\n",
        "        self,\n",
        "        parent_base_prompt: str,\n",
        "        parent_program: str,\n",
        "        parent_results: dict,\n",
        "        child_program: str,\n",
        "        child_results: dict,\n",
        "        existing_inspirations=list[dict]\n",
        "    ) -> list[str]:\n",
        "        \"\"\"\n",
        "        Produce a short inspiration idea when a new child performed worse than the parent.\n",
        "        The idea should hypothesize about the hidden evaluation criteria and suggest\n",
        "        prompt updates that could improve score next time.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "You are analyzing a code evolution experiment where the goal is to maximize an UNKNOWN (hidden) score.\n",
        "\n",
        "CONTEXT (Parent performed BETTER than Child):\n",
        "- Parent Base Prompt:\n",
        "{parent_base_prompt}\n",
        "\n",
        "- Parent Program:\n",
        "{parent_program}\n",
        "\n",
        "- Parent Results (incl. score):\n",
        "{parent_results}\n",
        "\n",
        "- Child Program (performed worse):\n",
        "{child_program}\n",
        "\n",
        "- Child Results (incl. score):\n",
        "{child_results}\n",
        "\n",
        "- Existing Updates/Inspirations:\n",
        "{existing_inspirations}\n",
        "\n",
        "TASK:\n",
        "1) Diagnose why the Child likely underperformed relative to the Parent.\n",
        "2) Hypothesize about the hidden scoring function (what it may reward/penalize).\n",
        "3) Recommend 1–3 concrete, testable updates to the BASE PROMPT that better target the hidden criteria. Do not repeat any of the existing updates.\n",
        "4) Summarize each recommendation as ONE concise \"inspiration idea\" that we can try next iteration. Do not repeat any of the existing inspirations.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "- Return ONLY a JSON list of inspirations.\n",
        "- Each inspiration should be an object of the form: {{ \"description\": \"<string>\" }}\n",
        "- The \"description\" should briefly capture the suspected scoring factors and how to update the base prompt.\n",
        "\"\"\"\n",
        "        response = await self.client.aio.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=prompt,\n",
        "            config={\n",
        "                \"response_mime_type\": \"application/json\",\n",
        "                \"response_schema\": list[Inspiration],\n",
        "            },\n",
        "        )\n",
        "        inspirations: list[Inspiration] = response.parsed\n",
        "        return [inspiration.description for inspiration in inspirations]\n",
        "\n",
        "\n",
        "    async def generate_new_base_prompt(self, base_prompt: str, parent_program: str, inspiration: str) -> str:\n",
        "        \"\"\"\n",
        "        Call Gemini to create a new base prompt given previous prompt, program, and inspiration.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "You are improving a code generation prompt.\n",
        "\n",
        "Previous Base Prompt:\n",
        "{base_prompt}\n",
        "\n",
        "Parent Program:\n",
        "{parent_program}\n",
        "\n",
        "Inspiration Idea:\n",
        "{inspiration[\"description\"]}\n",
        "\n",
        "Generate a new base prompt that incorporates the inspiration idea\n",
        "and would lead to an improved program.\n",
        "Return only the new prompt as plain text.\n",
        "\"\"\"\n",
        "        response = await self.client.aio.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "        new_prompt = response.candidates[0].content.parts[0].text\n",
        "        return new_prompt\n",
        "\n",
        "    async def generate_recommendation(self, base_prompt: str, program: str, results: dict, existing_inspirations: list[dict]) -> list[str]:\n",
        "        \"\"\"\n",
        "        Ask Gemini to suggest future improvements with explicit reasoning\n",
        "        about the UNKNOWN scoring function.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "You are reviewing a code generation experiment where the evaluation score is HIDDEN/UNKNOWN.\n",
        "Assume we want to maximize that hidden score and learn from this iteration.\n",
        "\n",
        "Base Prompt:\n",
        "{base_prompt}\n",
        "\n",
        "Generated Program:\n",
        "{program}\n",
        "\n",
        "Evaluation Results (incl. score):\n",
        "{results}\n",
        "\n",
        "- Existing Updates:\n",
        "{existing_inspirations}\n",
        "\n",
        "TASK:\n",
        "1) Infer what the hidden scoring function might reward or penalize based on the current outcome.\n",
        "2) Propose 1–3 concise, testable updates to the BASE PROMPT that exploit those hypotheses. Do not repeat any of the existing updates.\n",
        "3) Summarize each update as one short \"inspiration idea\" (1–3 sentences) to try next iteration.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "- Return ONLY a JSON list of inspirations.\n",
        "- Each inspiration should be an object of the form: {{ \"description\": \"<string>\" }}\n",
        "- The \"description\" should briefly capture the suspected scoring factors and how to update the base prompt.\n",
        "\"\"\"\n",
        "\n",
        "        response = await self.client.aio.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=prompt,\n",
        "            config={\n",
        "                \"response_mime_type\": \"application/json\",\n",
        "                \"response_schema\": list[Inspiration],\n",
        "            },\n",
        "        )\n",
        "        inspirations: list[Inspiration] = response.parsed\n",
        "        return [inspiration.description for inspiration in inspirations]\n",
        "\n",
        "# ------------------- Evaluator -------------------\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, eval_fn: Callable[[str, Any], dict]):\n",
        "        self.eval_fn = eval_fn\n",
        "\n",
        "    def execute(self, program: str) -> dict:\n",
        "        local_env = {}\n",
        "        try:\n",
        "            exec(program, {}, local_env)\n",
        "            result_value = local_env.get(\"result\", None)\n",
        "        except Exception as e:\n",
        "            return {\"score\": -1, \"error\": str(e)}\n",
        "\n",
        "        return self.eval_fn(program, result_value)\n",
        "\n",
        "# ------------------- Example Custom Eval -------------------\n",
        "\n",
        "def my_custom_eval(program: str, result: Any) -> dict:\n",
        "    score = result if isinstance(result, int) else 0\n",
        "    return {\"score\": score, \"output\": f\"Custom evaluated {program}\"}\n",
        "\n",
        "# ------------------- Enabler -------------------\n",
        "\n",
        "\n",
        "class Enabler:\n",
        "    def __init__(self, database, prompt_sampler, llm, evaluator, iterations=20):\n",
        "        self.database = database\n",
        "        self.prompt_sampler = prompt_sampler\n",
        "        self.llm = llm\n",
        "        self.evaluator = evaluator\n",
        "        self.iterations = iterations\n",
        "\n",
        "    # async def run(self):\n",
        "    #     for i in range(self.iterations):\n",
        "    #         iter_start = time.perf_counter()\n",
        "    #         print(f\"\\n=== Iteration {i+1}/{self.iterations} ===\")\n",
        "\n",
        "    #         parent_entry, inspirations, all_inspirations = self.database.sample()\n",
        "\n",
        "    #         parent_id = parent_entry[\"id\"]\n",
        "    #         print(f\"Chosen Parent Id {parent_id}\")\n",
        "\n",
        "    #         # Try each available inspiration\n",
        "    #         for inspiration in inspirations:\n",
        "    #             await self.explore_inspiration(parent_entry, inspiration, all_inspirations)\n",
        "\n",
        "    #         iter_end = time.perf_counter()\n",
        "    #         print(f\"Iteration {i+1} took {iter_end - iter_start:.2f} seconds\")\n",
        "\n",
        "\n",
        "    #         print(self.database.inspirations)\n",
        "    #         self.database.print_categories()\n",
        "\n",
        "    #     print(\"Final Categories: \\n\")\n",
        "    #     self.database.print_categories()\n",
        "\n",
        "    async def run(self):\n",
        "        for i in range(self.iterations):\n",
        "            iter_start = time.perf_counter()\n",
        "            print(f\"\\n=== Iteration {i+1}/{self.iterations} ===\")\n",
        "\n",
        "            parent_entry, inspirations, all_inspirations = self.database.sample()\n",
        "\n",
        "            parent_id = parent_entry[\"id\"]\n",
        "            print(f\"Chosen Parent Id {parent_id}\")\n",
        "\n",
        "            # Schedule all inspirations concurrently\n",
        "            tasks = [\n",
        "                asyncio.create_task(\n",
        "                    self.explore_inspiration(parent_entry, inspiration, all_inspirations)\n",
        "                )\n",
        "                for inspiration in inspirations\n",
        "            ]\n",
        "\n",
        "            # Wait for all to finish before moving to the next parent\n",
        "            await asyncio.gather(*tasks)\n",
        "\n",
        "            iter_end = time.perf_counter()\n",
        "            print(f\"Iteration {i+1} took {iter_end - iter_start:.2f} seconds\")\n",
        "\n",
        "            # Show current best\n",
        "            # best_entry = self.database.best()\n",
        "            # print(\"\\nBest so far:\", best_entry)\n",
        "            print(self.database.inspirations)\n",
        "            self.database.print_categories()\n",
        "\n",
        "        # Final best output\n",
        "        # best_entry = self.database.best()\n",
        "        print(\"Final Categories: \\n\")\n",
        "        self.database.print_categories()\n",
        "        # print(\"Best Evaluation Results:\\n\", best_entry)\n",
        "\n",
        "    async def explore_inspiration(self, parent_entry: dict, inspiration: dict, all_inspirations: list[dict]):\n",
        "        parent_program = parent_entry[\"program\"]\n",
        "        base_prompt = parent_entry[\"base_prompt\"]\n",
        "\n",
        "        # Capture the parent entry & score at the start of the iteration\n",
        "        parent_score = parent_entry[\"results\"].get(\"score\", 0)\n",
        "        # Build combined prompt (returns new base prompt as well)\n",
        "        combined_prompt, new_base_prompt = await self.prompt_sampler.build(\n",
        "            parent_program, base_prompt, inspiration\n",
        "        )\n",
        "\n",
        "        # Get LLM diff, apply it, evaluate the child\n",
        "        diff, _ = await self.llm.generate(combined_prompt)\n",
        "        child_program = self.llm.apply_diff(parent_program, diff)\n",
        "\n",
        "        results = self.evaluator.execute(child_program)\n",
        "        child_score = results.get(\"score\", 0)\n",
        "        if child_score < parent_score:\n",
        "          # Regression: create inspirations for the parent\n",
        "          parent_insps = await self.llm.generate_inspiration_regression(\n",
        "              parent_base_prompt=base_prompt,\n",
        "              parent_program=parent_program,\n",
        "              parent_results=parent_entry[\"results\"],\n",
        "              child_program=child_program,\n",
        "              child_results=results,\n",
        "              existing_inspirations=all_inspirations\n",
        "          )\n",
        "        else:\n",
        "            # No regression: still create inspirations for the parent\n",
        "            parent_insps = await self.llm.generate_recommendation(\n",
        "                base_prompt,\n",
        "                parent_program,\n",
        "                parent_entry[\"results\"],\n",
        "                existing_inspirations=all_inspirations\n",
        "            )\n",
        "\n",
        "        # Child always gets inspirations\n",
        "        child_insps = await self.llm.generate_recommendation(\n",
        "            new_base_prompt,\n",
        "            child_program,\n",
        "            results,\n",
        "            existing_inspirations=all_inspirations\n",
        "        )\n",
        "\n",
        "        embedding = await self.llm.embed_program(child_program)\n",
        "\n",
        "        result = self.database.add_result(child_program, results, new_base_prompt, embedding)\n",
        "\n",
        "        self.database.mark_inspiration_as_used(inspiration=inspiration, result_id=result[\"id\"])\n",
        "\n",
        "        print(f\"Inspirations Length Parent Insps:{len(parent_insps)}, Child Insps: {len(child_insps)}\")\n",
        "\n",
        "        for insp in parent_insps:\n",
        "            self.database.add_inspiration(\n",
        "                parent_result_id=parent_entry[\"id\"],\n",
        "                description=insp\n",
        "            )\n",
        "            print(\"\\nParent Inspiration:\\n\", insp)\n",
        "        for insp in child_insps:\n",
        "            self.database.add_inspiration(\n",
        "                parent_result_id=result[\"id\"],\n",
        "                description=insp\n",
        "            )\n",
        "            print(\"\\nChild Inspiration:\\n\", insp)\n",
        "\n",
        "# ------------------- Example Usage -------------------\n",
        "\n",
        "# Seed database\n",
        "database = Database()\n",
        "initial_program = \"\"\"\n",
        "def compute():\n",
        "    return 10\n",
        "\n",
        "result = compute()\n",
        "\"\"\"\n",
        "initial_base_prompt = \"\"\"\n",
        "This prompt will be used to generate code, but it is unclear exactly what the evaluation\n",
        "metric is. You should write code that you think would maximize some kind of score. Keep the code relatively short and simple.\n",
        "Focus on producing code that is correct, well-structured, and likely to achieve a high score.\n",
        "\"\"\"\n",
        "llm = LLM()\n",
        "\n",
        "embedding = await llm.embed_program(initial_program)\n",
        "\n",
        "database.add_result(initial_program, my_custom_eval(initial_program, 10), initial_base_prompt, embedding)\n",
        "database.add_inspiration(parent_result_id=1, description=\"\")\n",
        "\n",
        "# Initialize components\n",
        "\n",
        "prompt_sampler = PromptSampler(llm)\n",
        "evaluator = Evaluator(eval_fn=my_custom_eval)\n",
        "enabler = Enabler(database, prompt_sampler, llm, evaluator, iterations=2)\n",
        "\n",
        "# Run the evolution\n",
        "await enabler.run()\n"
      ]
    }
  ]
}