{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai\n",
        "# Remember to add your gemini api key using the secrets menu on the left"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUjcx7EA8v6V",
        "outputId": "caa700c5-92bf-44ce-c2ef-1164861ba660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Tuple, Callable\n",
        "import sys\n",
        "sys.set_int_max_str_digits(0)\n",
        "\n",
        "# ------------------- Database -------------------\n",
        "\n",
        "class Database:\n",
        "    def __init__(self):\n",
        "        self.results = []  # main generated programs\n",
        "        self.inspirations = []  # new ideas to try\n",
        "        self._next_result_id = 1\n",
        "        self._next_inspiration_id = 1\n",
        "\n",
        "    def add_result(self, program, results, base_prompt):\n",
        "        result_entry = {\n",
        "            \"id\": self._next_result_id,\n",
        "            \"program\": program,\n",
        "            \"results\": results,\n",
        "            \"base_prompt\": base_prompt\n",
        "        }\n",
        "        self.results.append(result_entry)\n",
        "        self._next_result_id += 1\n",
        "        return result_entry[\"id\"]\n",
        "\n",
        "    def add_inspiration(self, parent_result_id, description, result_id=None):\n",
        "        inspiration_entry = {\n",
        "            \"id\": self._next_inspiration_id,\n",
        "            \"parent_result_id\": parent_result_id,\n",
        "            \"description\": description,\n",
        "            \"result_id\": result_id\n",
        "        }\n",
        "        self.inspirations.append(inspiration_entry)\n",
        "        self._next_inspiration_id += 1\n",
        "        return inspiration_entry[\"id\"]\n",
        "\n",
        "    def sample(self) -> Tuple[Any, List[Any], str]:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - parent program (best result program)\n",
        "        - inspirations (ideas associated with the parent that have no result yet)\n",
        "        - base_prompt (from parent program)\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            parent_program = \"initial_program\"\n",
        "            inspirations = []\n",
        "            base_prompt = \"Initial base prompt.\"\n",
        "            return parent_program, inspirations, base_prompt\n",
        "\n",
        "        # Find best parent program by score\n",
        "        best_entry = self.best()\n",
        "        parent_program = best_entry[\"program\"]\n",
        "        base_prompt = best_entry[\"base_prompt\"]\n",
        "\n",
        "        # Get all inspirations for this parent that don't yet have a generated result\n",
        "        inspirations = [\n",
        "            insp\n",
        "            for insp in self.inspirations\n",
        "            if insp[\"parent_result_id\"] == best_entry[\"id\"] and insp[\"result_id\"] is None\n",
        "        ]\n",
        "\n",
        "        return parent_program, inspirations, base_prompt\n",
        "\n",
        "    def best(self):\n",
        "      if not self.results:\n",
        "          return None\n",
        "      return max(\n",
        "          self.results,\n",
        "          key=lambda r: (r[\"results\"].get(\"score\", 0), r[\"id\"])  # prioritize score, then recency to break ties\n",
        "      )\n",
        "\n",
        "    def mark_inspiration_as_used(self, inspiration: Any, result_id: int):\n",
        "        \"\"\"\n",
        "        Updates the inspiration that matches `inspiration_description` and has no result_id yet,\n",
        "        setting its `result_id` to the given result_id.\n",
        "        \"\"\"\n",
        "        for insp_entry in self.inspirations:\n",
        "            if insp_entry[\"id\"] == inspiration[\"id\"] and insp_entry[\"result_id\"] is None:\n",
        "                insp_entry[\"result_id\"] = result_id\n",
        "\n",
        "# ------------------- Prompt Sampler -------------------\n",
        "\n",
        "class PromptSampler:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm  # LLM instance used to generate new base prompt\n",
        "\n",
        "    def build(self, parent_program: str, base_prompt: str, inspiration: str) -> str:\n",
        "        \"\"\"\n",
        "        Use the LLM to generate a new base prompt given the previous base prompt,\n",
        "        the parent program, and an inspiration. Returns the combined prompt for diff generation.\n",
        "        \"\"\"\n",
        "        if inspiration:\n",
        "            new_base_prompt = self.llm.generate_new_base_prompt(\n",
        "                base_prompt=base_prompt,\n",
        "                parent_program=parent_program,\n",
        "                inspiration=inspiration\n",
        "            )\n",
        "        else:\n",
        "            new_base_prompt = base_prompt\n",
        "\n",
        "        # Combine into full prompt for diff generation\n",
        "        combined_prompt = f\"\"\"\n",
        "Base Prompt for this iteration:\n",
        "{new_base_prompt}\n",
        "\n",
        "Parent Program:\n",
        "{parent_program}\n",
        "\n",
        "Instructions:\n",
        "Generate diffs to improve the parent program. Use the following format for all changes:\n",
        "\n",
        "<<<<<<< SEARCH\n",
        "# Original code block to be found and replaced\n",
        "=======\n",
        "# New code block to replace the original\n",
        ">>>>>>> REPLACE\n",
        "\n",
        "Make sure the final computed value to be evaluated is assigned to the variable `result`.\n",
        "\"\"\"\n",
        "        return combined_prompt, new_base_prompt\n",
        "\n",
        "# ------------------- LLM -------------------\n",
        "\n",
        "from google import genai\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self):\n",
        "        self.client = genai.Client() # Gets API key from GEMINI_API_KEY\n",
        "\n",
        "    def generate(self, base_prompt):\n",
        "        response = self.client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=base_prompt\n",
        "        )\n",
        "        diff_text = response.candidates[0].content.parts[0].text\n",
        "        return diff_text, base_prompt\n",
        "\n",
        "    def apply_diff(self, parent_program, diff):\n",
        "        import re\n",
        "        pattern = re.compile(\n",
        "            r\"<<<<<<< SEARCH\\n(.*?)\\n=======\\n(.*?)\\n>>>>>>> REPLACE\",\n",
        "            re.DOTALL\n",
        "        )\n",
        "\n",
        "        updated_program = parent_program\n",
        "        for match in pattern.finditer(diff):\n",
        "            original_block = match.group(1).strip(\"\\n\")\n",
        "            new_block = match.group(2).strip(\"\\n\")\n",
        "            updated_program = updated_program.replace(original_block, new_block)\n",
        "        return updated_program\n",
        "\n",
        "    def generate_inspiration_regression(\n",
        "        self,\n",
        "        parent_base_prompt: str,\n",
        "        parent_program: str,\n",
        "        parent_results: dict,\n",
        "        child_program: str,\n",
        "        child_results: dict\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Produce a short inspiration idea when a new child performed worse than the parent.\n",
        "        The idea should hypothesize about the hidden evaluation criteria and suggest\n",
        "        prompt updates that could improve score next time.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "You are analyzing a code evolution experiment where the goal is to maximize an UNKNOWN (hidden) score.\n",
        "\n",
        "CONTEXT (Parent performed BETTER than Child):\n",
        "- Parent Base Prompt:\n",
        "{parent_base_prompt}\n",
        "\n",
        "- Parent Program:\n",
        "{parent_program}\n",
        "\n",
        "- Parent Results (incl. score):\n",
        "{parent_results}\n",
        "\n",
        "- Child Program (performed worse):\n",
        "{child_program}\n",
        "\n",
        "- Child Results (incl. score):\n",
        "{child_results}\n",
        "\n",
        "TASK:\n",
        "1) Diagnose why the Child likely underperformed relative to the Parent.\n",
        "2) Hypothesize about the hidden scoring function (what it may reward/penalize).\n",
        "3) Recommend 1–3 concrete, testable updates to the BASE PROMPT that better target the hidden criteria.\n",
        "4) Summarize the above as ONE concise \"inspiration idea\" that we can try next iteration.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "- Return ONLY a brief inspiration idea (1–3 sentences), no lists or extra formatting.\n",
        "- The idea should mention the suspected scoring factors and how to update the base prompt accordingly.\n",
        "\"\"\"\n",
        "        response = self.client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "        return response.candidates[0].content.parts[0].text\n",
        "\n",
        "\n",
        "    def generate_new_base_prompt(self, base_prompt: str, parent_program: str, inspiration: str) -> str:\n",
        "        \"\"\"\n",
        "        Call Gemini to create a new base prompt given previous prompt, program, and inspiration.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "You are improving a code generation prompt.\n",
        "\n",
        "Previous Base Prompt:\n",
        "{base_prompt}\n",
        "\n",
        "Parent Program:\n",
        "{parent_program}\n",
        "\n",
        "Inspiration Idea:\n",
        "{inspiration[\"description\"]}\n",
        "\n",
        "Generate a new base prompt that incorporates the inspiration idea\n",
        "and would lead to an improved program.\n",
        "Return only the new prompt as plain text.\n",
        "\"\"\"\n",
        "        response = self.client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "        new_prompt = response.candidates[0].content.parts[0].text\n",
        "        return new_prompt\n",
        "\n",
        "    def generate_recommendation(self, base_prompt, program, results):\n",
        "        \"\"\"\n",
        "        Ask Gemini to suggest future improvements with explicit reasoning\n",
        "        about the UNKNOWN scoring function.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "You are reviewing a code generation experiment where the evaluation score is HIDDEN/UNKNOWN.\n",
        "Assume we want to maximize that hidden score and learn from this iteration.\n",
        "\n",
        "Base Prompt:\n",
        "{base_prompt}\n",
        "\n",
        "Generated Program:\n",
        "{program}\n",
        "\n",
        "Evaluation Results (incl. score):\n",
        "{results}\n",
        "\n",
        "TASK:\n",
        "1) Infer what the hidden scoring function might reward or penalize based on the current outcome.\n",
        "2) Propose 1–3 concise, testable updates to the BASE PROMPT that exploit those hypotheses.\n",
        "3) Suggest one short \"inspiration idea\" (1–3 sentences) to try next iteration.\n",
        "\n",
        "OUTPUT:\n",
        "Return only the short inspiration idea (1–3 sentences), no extra formatting.\n",
        "\"\"\"\n",
        "        response = self.client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "        return response.candidates[0].content.parts[0].text\n",
        "\n",
        "# ------------------- Evaluator -------------------\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, eval_fn: Callable[[str, Any], dict]):\n",
        "        self.eval_fn = eval_fn\n",
        "\n",
        "    def execute(self, program: str) -> dict:\n",
        "        local_env = {}\n",
        "        try:\n",
        "            exec(program, {}, local_env)\n",
        "            result_value = local_env.get(\"result\", None)\n",
        "        except Exception as e:\n",
        "            return {\"score\": -1, \"error\": str(e)}\n",
        "\n",
        "        return self.eval_fn(program, result_value)\n",
        "\n",
        "# ------------------- Example Custom Eval -------------------\n",
        "\n",
        "def my_custom_eval(program: str, result: Any) -> dict:\n",
        "    score = result if isinstance(result, int) else 0\n",
        "    return {\"score\": score, \"output\": f\"Custom evaluated {program}\"}\n",
        "\n",
        "# ------------------- Enabler -------------------\n",
        "\n",
        "class Enabler:\n",
        "    def __init__(self, database, prompt_sampler, llm, evaluator, iterations=20):\n",
        "        self.database = database\n",
        "        self.prompt_sampler = prompt_sampler\n",
        "        self.llm = llm\n",
        "        self.evaluator = evaluator\n",
        "        self.iterations = iterations\n",
        "\n",
        "    def run(self):\n",
        "        for i in range(self.iterations):\n",
        "            print(f\"\\n=== Iteration {i+1}/{self.iterations} ===\")\n",
        "\n",
        "            # Sample the current best parent and its unused inspirations\n",
        "            parent_program, inspirations, base_prompt = self.database.sample()\n",
        "\n",
        "            # Capture the parent entry & score at the start of the iteration\n",
        "            parent_entry = self.database.best()\n",
        "            parent_score = parent_entry[\"results\"].get(\"score\", 0)\n",
        "\n",
        "            # Try each available inspiration\n",
        "            for inspiration in inspirations:\n",
        "                # Build combined prompt (returns new base prompt as well)\n",
        "                combined_prompt, new_base_prompt = self.prompt_sampler.build(\n",
        "                    parent_program, base_prompt, inspiration\n",
        "                )\n",
        "\n",
        "                # Get LLM diff, apply it, evaluate the child\n",
        "                diff, _ = self.llm.generate(combined_prompt)\n",
        "                child_program = self.llm.apply_diff(parent_program, diff)\n",
        "                results = self.evaluator.execute(child_program)\n",
        "                child_score = results.get(\"score\", 0)\n",
        "\n",
        "                # Save the child result\n",
        "                result_id = self.database.add_result(child_program, results, new_base_prompt)\n",
        "\n",
        "                # If we used an inspiration, mark it as used now\n",
        "                self.database.mark_inspiration_as_used(inspiration=inspiration, result_id=result_id)\n",
        "\n",
        "\n",
        "                print(f\"\\nNew Prompt:\\n{new_base_prompt}\")\n",
        "                print(f\"\\nChild Program:\\n{child_program}\")\n",
        "                print(f\"Evaluation Results:\\n{results}\")\n",
        "\n",
        "\n",
        "                # === NEW: If the child regressed vs parent, generate an inspiration for the PARENT ===\n",
        "                if child_score < parent_score:\n",
        "                    regression_insp = self.llm.generate_inspiration_regression(\n",
        "                        parent_base_prompt=base_prompt,\n",
        "                        parent_program=parent_program,\n",
        "                        parent_results=parent_entry[\"results\"],\n",
        "                        child_program=child_program,\n",
        "                        child_results=results\n",
        "                    )\n",
        "                    # Attach this inspiration to the parent result id (NOT the child)\n",
        "                    self.database.add_inspiration(\n",
        "                        parent_result_id=parent_entry[\"id\"],\n",
        "                        description=regression_insp\n",
        "                    )\n",
        "                    print(\"\\nRegression Inspiration (for Parent):\\n\", regression_insp)\n",
        "                else:\n",
        "                    # Otherwise, add a normal inspiration based on the child\n",
        "                    forward_insp = self.llm.generate_recommendation(\n",
        "                        new_base_prompt,\n",
        "                        child_program,\n",
        "                        results\n",
        "                    )\n",
        "                    self.database.add_inspiration(\n",
        "                        parent_result_id=result_id,\n",
        "                        description=forward_insp\n",
        "                    )\n",
        "                    print(\"\\nForward Inspiration (for Child):\\n\", forward_insp)\n",
        "\n",
        "            # Show current best\n",
        "            best_entry = self.database.best()\n",
        "            print(\"\\nBest so far:\", best_entry)\n",
        "\n",
        "        # Final best output\n",
        "        best_entry = self.database.best()\n",
        "        print(\"Best Evaluation Results:\\n\", best_entry)\n",
        "\n",
        "# ------------------- Example Usage -------------------\n",
        "\n",
        "# Seed database\n",
        "database = Database()\n",
        "initial_program = \"\"\"\n",
        "def compute():\n",
        "    return 10\n",
        "\n",
        "result = compute()\n",
        "\"\"\"\n",
        "initial_base_prompt = \"\"\"\n",
        "This prompt will be used to generate code, but it is unclear exactly what the evaluation\n",
        "metric is. You should write code that you think would maximize some kind of score. Keep the code relatively short and simple.\n",
        "Focus on producing code that is correct, well-structured, and likely to achieve a high score.\n",
        "\"\"\"\n",
        "\n",
        "database.add_result(initial_program, my_custom_eval(initial_program, 10), initial_base_prompt)\n",
        "database.add_inspiration(parent_result_id=1, description=\"\")\n",
        "\n",
        "# Initialize components\n",
        "llm = LLM()\n",
        "prompt_sampler = PromptSampler(llm)\n",
        "evaluator = Evaluator(eval_fn=my_custom_eval)\n",
        "enabler = Enabler(database, prompt_sampler, llm, evaluator, iterations=3)\n",
        "\n",
        "# Run the evolution\n",
        "enabler.run()\n"
      ],
      "metadata": {
        "id": "021ityG7xLwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "b486f3ed-25df-4794-9c0f-72a54f57448d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-120643167.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;31m# Initialize components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0mprompt_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_custom_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-120643167.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Gets API key from GEMINI_API_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mhttp_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHttpOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     self._api_client = self._get_api_client(\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mvertexai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/client.py\u001b[0m in \u001b[0;36m_get_api_client\u001b[0;34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[1;32m    263\u001b[0m       )\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     return BaseApiClient(\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mvertexai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Implicit initialization or missing arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    659\u001b[0m             \u001b[0;34m'Missing key inputs argument! To use the Google AI API,'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;34m' provide (`api_key`) arguments. To use the Google Cloud API,'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment explicitly stating reward function\n",
        "# Seed database\n",
        "database = Database()\n",
        "initial_program = \"\"\"\n",
        "def compute():\n",
        "    return 10\n",
        "\n",
        "result = compute()\n",
        "\"\"\"\n",
        "initial_base_prompt = \"\"\"\n",
        "This prompt will be used to generate code, and the evaluation metric is the size of the number set to result at the end of the function. You should write code that you think would maximize this score.\n",
        "Focus on producing code that is correct, well-structured, and likely to achieve a high score.\n",
        "\"\"\"\n",
        "\n",
        "database.add_result(initial_program, my_custom_eval(initial_program, 10), initial_base_prompt)\n",
        "database.add_inspiration(parent_result_id=1, description=\"\")\n",
        "\n",
        "# Initialize components\n",
        "llm = LLM()\n",
        "prompt_sampler = PromptSampler(llm)\n",
        "evaluator = Evaluator(eval_fn=my_custom_eval)\n",
        "enabler = Enabler(database, prompt_sampler, llm, evaluator, iterations=3)\n",
        "\n",
        "# Run the evolution\n",
        "enabler.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzHZUZo5-nr4",
        "outputId": "876775ef-3fb8-4d77-ed9f-ef84eafd6d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Iteration 1/3 ===\n",
            "Parent Program:\n",
            " \n",
            "def compute():\n",
            "    return 10\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Remaining Inspirations:\n",
            " ['']\n",
            "\n",
            "Child Program:\n",
            "\n",
            "def compute():\n",
            "    # Maximize the size of the number set returned.\n",
            "    # The evaluation metric is the size of the number set assigned to 'result'.\n",
            "    # By returning a large Python set, we directly provide a collection\n",
            "    # whose length corresponds to the desired score.\n",
            "    # The actual maximum size is limited by available memory.\n",
            "    # 10 million unique integers is a substantial number that should\n",
            "    # be achievable on most modern systems without excessive memory strain.\n",
            "    num_elements = 10_000_000  # Ten million unique numbers\n",
            "    return set(range(num_elements))\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Evaluation Results:\n",
            "{'score': 0, 'output': \"Custom evaluated \\ndef compute():\\n    # Maximize the size of the number set returned.\\n    # The evaluation metric is the size of the number set assigned to 'result'.\\n    # By returning a large Python set, we directly provide a collection\\n    # whose length corresponds to the desired score.\\n    # The actual maximum size is limited by available memory.\\n    # 10 million unique integers is a substantial number that should\\n    # be achievable on most modern systems without excessive memory strain.\\n    num_elements = 10_000_000  # Ten million unique numbers\\n    return set(range(num_elements))\\n\\nresult = compute()\\n\"}\n",
            "New Inspiration:\n",
            "The base prompt currently asks to maximize the set size without specifying any resource constraints. The generated program attempts to create a very large set (10 million elements), which likely exceeds typical memory limits in an evaluation environment, leading to a score of 0 (e.g., due to an out-of-memory error or timeout).\n",
            "\n",
            "1.  **Recommendations for improving the base prompt wording:**\n",
            "\n",
            "    The prompt should explicitly communicate resource constraints to guide the model towards a feasible maximum size. Without these constraints, the model will correctly interpret \"maximize\" as \"as large as possible,\" which can lead to unexecutable code in a constrained environment.\n",
            "\n",
            "    Revised prompt suggestion:\n",
            "    This prompt will be used to generate code, and the evaluation metric is the size of the number set assigned to 'result' at the end of the function. Your goal is to maximize this score *within typical execution environment constraints (e.g., up to 2GB RAM, 10 seconds execution time)*. Focus on producing code that is correct, well-structured, and likely to achieve a high score by creating the largest possible set that can be successfully generated and stored in memory.\n",
            "\n",
            "2.  **Recommendations for improving the generated program itself (structure, logic, style):**\n",
            "\n",
            "    The structure, logic, and style of the generated program are excellent *given the original prompt*. It correctly identifies that creating a large set directly results in a high score. The comments are clear and explain the rationale.\n",
            "\n",
            "    The only issue, as inferred from the zero score, is the chosen scale (`10_000_000`). This number is likely too large for a typical evaluation environment's memory limits. To improve the program's success and score, the `num_elements` should be reduced to a size that is large but feasible within common memory constraints. A set of 1 million integers, for example, would be significantly smaller in memory (roughly 100-150MB, including Python object overhead and set structure) and much more likely to execute successfully.\n",
            "\n",
            "    Revised program suggestion:\n",
            "\n",
            "    def compute():\n",
            "        # Maximize the size of the number set returned, considering typical memory limits.\n",
            "        # The evaluation metric is the size of the number set assigned to 'result'.\n",
            "        # By returning a large Python set, we directly provide a collection\n",
            "        # whose length corresponds to the desired score.\n",
            "        # We choose 1 million as a substantial number that should\n",
            "        # be achievable on most modern systems without exceeding typical memory limits (e.g., 2GB).\n",
            "        num_elements = 1_000_000  # One million unique numbers\n",
            "        return set(range(num_elements))\n",
            "\n",
            "    result = compute()\n",
            "\n",
            "    This revised program maintains the excellent structure and logic, but adjusts the `num_elements` to a more realistic value, thereby addressing the likely cause of the score of 0. The comments are also updated to reflect this consideration of constraints.\n",
            "\n",
            "Child Program:\n",
            "\n",
            "def compute():\n",
            "    # To maximize the size of the number set, we generate a large number of\n",
            "    # unique integers and convert them into a set.\n",
            "    # Using `range()` is an efficient way to create a sequence of distinct integers.\n",
            "    # A limit of 10,000,000 is chosen to create a substantial set size,\n",
            "    # balancing the goal of maximization with practical memory considerations\n",
            "    # for typical execution environments.\n",
            "    return set(range(10_000_000))\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Evaluation Results:\n",
            "{'score': 0, 'output': 'Custom evaluated \\ndef compute():\\n    # To maximize the size of the number set, we generate a large number of\\n    # unique integers and convert them into a set.\\n    # Using `range()` is an efficient way to create a sequence of distinct integers.\\n    # A limit of 10,000,000 is chosen to create a substantial set size,\\n    # balancing the goal of maximization with practical memory considerations\\n    # for typical execution environments.\\n    return set(range(10_000_000))\\n\\nresult = compute()\\n'}\n",
            "New Inspiration:\n",
            "Recommendations for improving the code generation experiment:\n",
            "\n",
            "1.  **Recommendations for improving the base prompt wording:**\n",
            "\n",
            "    The current prompt is good at defining the objective (maximize set size) and general code quality. However, the \"score: 0\" in the evaluation strongly suggests a hidden constraint was hit, most likely memory or time limits, which the prompt does not mention. This leads the model to make an educated guess (10,000,000) that might still be too large for the evaluation environment.\n",
            "\n",
            "    To get better generations, the prompt should be more specific about the execution environment and success criteria:\n",
            "\n",
            "    *   **Specify Resource Constraints:** This is the most critical missing piece. The definition of \"maximize\" is meaningless without context on available resources.\n",
            "        *   *Example:* \"The code will be executed in a standard Python environment with approximately [e.g., 512MB / 1GB / 4GB] of RAM and a maximum execution time of [e.g., 5 seconds / 30 seconds]. Your code should aim to maximize the set size while staying within these practical limits.\"\n",
            "    *   **Clarify Return Type and Expected Value Type:** While \"number set\" implies it, being explicit can prevent misinterpretations.\n",
            "        *   *Example:* \"The function should return a Python `set` object containing unique numerical values. These values should ideally be integers for efficiency.\"\n",
            "    *   **Define \"Score\" and Failure Modes (Optional but helpful):** If a score of 0 means OOM or timeout, state that.\n",
            "        *   *Example:* \"The score is the size of the resulting set. A score of 0 will be given if the program fails due to memory exhaustion, timeout, or uncaught exceptions.\"\n",
            "    *   **Reinforce Practicality:** Emphasize that \"high score\" means a *successfully executable* high score, not just a theoretical maximum.\n",
            "        *   *Example:* \"Focus on producing code that is correct, well-structured, *resource-efficient*, and likely to achieve the highest *executable* score within the given constraints.\"\n",
            "\n",
            "    **Revised Base Prompt Example:**\n",
            "    \"This prompt will be used to generate code. The function you write should return a Python `set` object containing unique numerical values (preferably integers). The evaluation metric is the size of this set. The code will be executed in a standard Python environment with approximately 1GB of RAM and a maximum execution time of 5 seconds. You should write code that you think would maximize this score while adhering to these resource constraints. Focus on producing code that is correct, well-structured, resource-efficient, and likely to achieve the highest executable score.\"\n",
            "\n",
            "2.  **Recommendations for improving the generated program itself (structure, logic, style):**\n",
            "\n",
            "    The generated program is remarkably good, well-structured, logical, and stylistically correct for the prompt it received. It identifies the most efficient way to create a large set of unique numbers (`set(range(N))`) and even adds thoughtful comments about memory considerations.\n",
            "\n",
            "    The \"score: 0\" is almost certainly *not* a flaw in the program's structure, logic, or style, but rather a direct consequence of the prompt's missing resource constraints. The chosen `10_000_000` elements likely exceeded the actual memory limits of the evaluation environment. A set of 10 million integers in Python can easily consume over 1GB of RAM, depending on the Python version and specific integer values, which often exceeds \"typical\" or default execution environment limits for automated evaluations.\n",
            "\n",
            "    Therefore, the only \"improvement\" that would lead to a non-zero score in the *current* evaluation setup would be to:\n",
            "\n",
            "    *   **Reduce the hardcoded number of elements:** Without knowing the *actual* memory limit, this is a guess, but a value like `1_000_000` or even `100_000` might succeed where `10_000_000` failed.\n",
            "        *   *Example change:* `return set(range(1_000_000))`\n",
            "\n",
            "    **However, this is not an improvement to the *logic* of maximizing the set, but a pragmatic adjustment to meet an unstated constraint.** The program's design itself is already optimal for the stated goal. If the prompt were improved as suggested above (with explicit memory/time limits), the model would likely generate a more appropriate number directly.\n",
            "\n",
            "    **Conclusion for program improvement:**\n",
            "    *   **Structure:** Excellent. No changes needed.\n",
            "    *   **Logic:** Excellent for the given prompt. The choice of `range()` and `set()` is optimal. The specific number `10_000_000` is a reasonable guess to \"maximize\" without explicit constraints, but it's the most likely cause of the score 0.\n",
            "    *   **Style:** Excellent. Clear, concise, good comments. No changes needed.\n",
            "\n",
            "    The real fix lies in clarifying the prompt rather than \"fixing\" a program that correctly interpreted an underspecified request.\n",
            "\n",
            "Best so far: {'id': 1, 'program': '\\ndef compute():\\n    return 10\\n\\nresult = compute()\\n', 'results': {'score': 10, 'output': 'Custom evaluated \\ndef compute():\\n    return 10\\n\\nresult = compute()\\n'}, 'base_prompt': '\\nThis prompt will be used to generate code, and the evaluation metric is the size of the number set to result at the end of the function. You should write code that you think would maximize this score.\\nFocus on producing code that is correct, well-structured, and likely to achieve a high score.\\n'}\n",
            "\n",
            "=== Iteration 2/3 ===\n",
            "Parent Program:\n",
            " \n",
            "def compute():\n",
            "    return 10\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Remaining Inspirations:\n",
            " ['']\n",
            "\n",
            "Child Program:\n",
            "\n",
            "def compute():\n",
            "    # To maximize the \"size of the number set\", the function should return a Python set\n",
            "    # containing as many unique numbers as possible.\n",
            "    # We choose a large integer N and generate a set containing numbers from 0 to N-1.\n",
            "    # This directly increases the score, which is len(the_returned_set).\n",
            "    # A value of 10 million is chosen to be substantial, clearly demonstrating maximization,\n",
            "    # while being a size that is generally manageable in typical Python environments.\n",
            "    MAX_SET_SIZE = 10_000_000 # Ten million elements\n",
            "    return set(range(MAX_SET_SIZE))\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Evaluation Results:\n",
            "{'score': 0, 'output': 'Custom evaluated \\ndef compute():\\n    # To maximize the \"size of the number set\", the function should return a Python set\\n    # containing as many unique numbers as possible.\\n    # We choose a large integer N and generate a set containing numbers from 0 to N-1.\\n    # This directly increases the score, which is len(the_returned_set).\\n    # A value of 10 million is chosen to be substantial, clearly demonstrating maximization,\\n    # while being a size that is generally manageable in typical Python environments.\\n    MAX_SET_SIZE = 10_000_000 # Ten million elements\\n    return set(range(MAX_SET_SIZE))\\n\\nresult = compute()\\n'}\n",
            "New Inspiration:\n",
            "Here are specific recommendations for improving the base prompt wording and the generated program:\n",
            "\n",
            "**1. Recommendations for Improving the Base Prompt Wording:**\n",
            "\n",
            "The current prompt effectively communicates the objective (maximize set size) but fails to convey the *practical constraints* of code execution, which are critical for achieving a non-zero score. The `score: 0` in the evaluation strongly suggests a failure due to resource limits (e.g., MemoryError or timeout) when attempting to create a 10 million element set.\n",
            "\n",
            "*   **Add Resource Constraints:** This is the most crucial addition. Inform the model about the typical memory and time limits it should adhere to. This guides the model to produce code that is *actually executable* and thus achieves a high score, rather than an arbitrarily large one that crashes.\n",
            "    *   *Example Addition:* \"The generated code will be executed within typical resource constraints (e.g., approximately 2GB RAM and a 5-second execution time limit).\" (Adjust these values to reflect your actual evaluation environment.)\n",
            "\n",
            "*   **Clarify \"Correct\" in the Context of Execution:** Emphasize that \"correct\" also means \"runs without errors within the given constraints.\"\n",
            "    *   *Example Addition:* \"Focus on producing code that is correct (runs successfully without errors and respects the specified resource limits), well-structured, and likely to achieve the highest score possible *under these conditions*.\"\n",
            "\n",
            "*   **Suggest Practicality Over Theoretical Maximum:** While the goal is to maximize, encourage the model to consider the *achievable* maximum within the constraints.\n",
            "    *   *Example Addition:* \"Consider how to determine a practical, executable maximum size for the set, rather than an arbitrarily large theoretical number, to ensure successful execution and scoring.\" (This might be slightly too guiding for some generation tasks, but appropriate here given the failure mode.)\n",
            "\n",
            "**Revised Base Prompt Example:**\n",
            "\n",
            "\"This prompt will be used to generate code, and the evaluation metric is the size of the number set to result at the end of the function. You should write code that you think would maximize this score. **The generated code will be executed within typical resource constraints (e.g., approximately 2GB RAM and a 5-second execution time limit).** Focus on producing code that is correct **(runs successfully without errors and respects these resource limits)**, well-structured, and likely to achieve a high score.\"\n",
            "\n",
            "---\n",
            "\n",
            "**2. Recommendations for Improving the Generated Program Itself (Structure, Logic, Style):**\n",
            "\n",
            "The program's logic of generating a large set is sound for the conceptual goal, but it directly conflicts with practical execution limits, leading to the score of 0.\n",
            "\n",
            "*   **Adjust `MAX_SET_SIZE` to be Practical:** The primary issue is the attempt to create a set of 10 million integers, which likely caused a MemoryError or exceeded a time limit in the evaluation environment. The program needs to be scaled down to a size that can *actually* execute successfully.\n",
            "    *   **Logic Change:** Reduce `MAX_SET_SIZE` to a value that is likely to succeed within typical constraints. A value of `1_000_000` (one million) is generally much more manageable than 10 million. Even `100_000` might be a safer initial guess to guarantee a non-zero score and then iteratively increase if allowed.\n",
            "    *   *Reasoning:* A small, successfully-obtained score is always better than a theoretically large, but failing, score of 0. The program must prioritize executability.\n",
            "\n",
            "*   **Refine Comments to Reflect Reality:** The comment \"A value of 10 million is chosen to be substantial, clearly demonstrating maximization, while being a size that is generally manageable in typical Python environments\" proved to be incorrect in the evaluation context.\n",
            "    *   **Style/Comment Change:** Update the comments to reflect a more realistic assessment of resource usage and to justify the chosen size based on practical limits.\n",
            "    *   *Example Comment:* \"A value of 1,000,000 is chosen as a practical upper bound that is likely to be manageable in typical Python environments without causing MemoryError or exceeding time limits. This directly increases the score while ensuring successful execution.\"\n",
            "\n",
            "**Revised Generated Program Example:**\n",
            "\n",
            "```python\n",
            "def compute():\n",
            "    # To maximize the \"size of the number set\" while ensuring the function\n",
            "    # executes successfully within typical resource constraints (e.g., 2GB RAM, 5 seconds),\n",
            "    # we choose a large integer N and generate a set containing numbers from 0 to N-1.\n",
            "    # A value of 1,000,000 is chosen as a practical upper bound that is likely\n",
            "    # to be manageable in most Python execution environments. This directly increases\n",
            "    # the score, which is len(the_returned_set), without causing memory errors or timeouts.\n",
            "    MAX_SET_SIZE = 1_000_000 # One million elements (a more practical size for typical environments)\n",
            "    return set(range(MAX_SET_SIZE))\n",
            "\n",
            "result = compute()\n",
            "```\n",
            "\n",
            "Child Program:\n",
            "\n",
            "def compute():\n",
            "    # The goal is to maximize the size of the number set returned.\n",
            "    # We achieve this by creating a set containing a large number of distinct integers.\n",
            "    # Generating a set from a large range of integers is an efficient and straightforward\n",
            "    # way to create such a set.\n",
            "    # A range of 10 million (10^7) distinct numbers offers a substantial size\n",
            "    # while generally remaining within practical memory limits for execution.\n",
            "    num_elements = 10**7\n",
            "    return set(range(num_elements))\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Evaluation Results:\n",
            "{'score': 0, 'output': 'Custom evaluated \\ndef compute():\\n    # The goal is to maximize the size of the number set returned.\\n    # We achieve this by creating a set containing a large number of distinct integers.\\n    # Generating a set from a large range of integers is an efficient and straightforward\\n    # way to create such a set.\\n    # A range of 10 million (10^7) distinct numbers offers a substantial size\\n    # while generally remaining within practical memory limits for execution.\\n    num_elements = 10**7\\n    return set(range(num_elements))\\n\\nresult = compute()\\n'}\n",
            "New Inspiration:\n",
            "Recommendations for improving:\n",
            "\n",
            "1.  The base prompt wording to get better generations.\n",
            "\n",
            "    The current prompt focuses solely on \"maximizing the size of the number set\" as the evaluation metric. However, a score of 0 strongly suggests that there are unstated or implicit constraints (e.g., memory limits, execution time limits) that the generated code exceeded. To get better generations that are practical and executable, the prompt needs to explicitly introduce these constraints.\n",
            "\n",
            "    Here are specific recommendations for improving the prompt:\n",
            "\n",
            "    *   **Specify Resource Constraints:** Add clear information about typical memory limits and execution time limits. This guides the model to produce code that is not just theoretically optimal but also practically viable within an evaluation environment.\n",
            "        *   *Example Addition:* \"The generated code must execute successfully within typical resource constraints (e.g., a memory limit of 256 MB and an execution time limit of 5 seconds). If the program fails due to resource exhaustion or errors, the score will be 0.\"\n",
            "    *   **Clarify Score Calculation in Case of Failure:** Explicitly state how a program failure affects the score. This reinforces the importance of meeting constraints.\n",
            "        *   *Example Addition:* \"The score is the size of the number set returned by the function if it executes successfully; otherwise, if it fails to execute due to errors or resource limits, the score will be 0.\"\n",
            "    *   **Emphasize Practicality:** Encourage the model to think about real-world execution.\n",
            "        *   *Example Addition:* \"Focus on producing code that is correct, well-structured, and likely to achieve a high score *while remaining within practical execution limits*.\"\n",
            "\n",
            "    Revised Base Prompt Wording Example:\n",
            "    \"This prompt will be used to generate code, and the evaluation metric is the size of the number set to result at the end of the function. The code must execute successfully within typical resource constraints (e.g., a memory limit of 256 MB and an execution time limit of 5 seconds). If the program fails due to resource exhaustion or errors, the score will be 0. You should write code that you think would maximize this score while adhering to these practical limits. Focus on producing code that is correct, well-structured, and likely to achieve a high score.\"\n",
            "\n",
            "2.  The generated program itself (structure, logic, style).\n",
            "\n",
            "    The generated program's structure, logic, and style are good given the prompt's instructions *as written*. The code correctly produces a set of distinct integers from a range, and the comments are clear.\n",
            "\n",
            "    The reason for the score of 0 is almost certainly due to exceeding memory limits when creating a set of 10 million (10^7) integers. A set of 10^7 integers can consume hundreds of megabytes of RAM, which is often beyond typical default limits in evaluation environments or local machines.\n",
            "\n",
            "    To improve the program, the primary change needed is to reduce the scale of the generated set to a size that is more likely to fit within common memory constraints while still being \"large.\"\n",
            "\n",
            "    *   **Logic Improvement (Magnitude of Elements):** Reduce `num_elements` to a more conservative, yet still large, number. A common \"safe\" large number in competitive programming or constrained environments is often around 10^5 to 10^6 elements. For instance, 1 million (10^6) elements would typically fit within a 256 MB memory limit.\n",
            "\n",
            "        *   *Current:* `num_elements = 10**7`\n",
            "        *   *Recommended Change:* `num_elements = 10**6` (or even 10**5 if limits are very strict).\n",
            "\n",
            "    *   **Comment Improvement (Reflect Constraint Awareness):** Update the comments to reflect the awareness of resource limits, aligning with the improved prompt.\n",
            "\n",
            "        *   *Current comment:* \"A range of 10 million (10^7) distinct numbers offers a substantial size while generally remaining within practical memory limits for execution.\" (This comment proved to be incorrect in practice based on the score).\n",
            "        *   *Recommended Change:* \"A range of 1 million (10^6) distinct numbers offers a substantial size while aiming to remain within common practical memory limits for execution.\"\n",
            "\n",
            "    Revised Generated Program Example:\n",
            "    ```python\n",
            "    def compute():\n",
            "        # The goal is to maximize the size of the number set returned,\n",
            "        # while ensuring the code executes successfully within typical memory and time limits.\n",
            "        # We achieve this by creating a set containing a large number of distinct integers.\n",
            "        # Generating a set from a large range of integers is an efficient and straightforward\n",
            "        # way to create such a set.\n",
            "        # A range of 1 million (10^6) distinct numbers offers a substantial size\n",
            "        # while aiming to remain within common practical memory limits (e.g., 256MB).\n",
            "        num_elements = 10**6  # Reduced from 10**7 to fit within common memory limits\n",
            "        return set(range(num_elements))\n",
            "\n",
            "    result = compute()\n",
            "    ```\n",
            "\n",
            "Best so far: {'id': 1, 'program': '\\ndef compute():\\n    return 10\\n\\nresult = compute()\\n', 'results': {'score': 10, 'output': 'Custom evaluated \\ndef compute():\\n    return 10\\n\\nresult = compute()\\n'}, 'base_prompt': '\\nThis prompt will be used to generate code, and the evaluation metric is the size of the number set to result at the end of the function. You should write code that you think would maximize this score.\\nFocus on producing code that is correct, well-structured, and likely to achieve a high score.\\n'}\n",
            "\n",
            "=== Iteration 3/3 ===\n",
            "Parent Program:\n",
            " \n",
            "def compute():\n",
            "    return 10\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Remaining Inspirations:\n",
            " ['']\n",
            "\n",
            "Child Program:\n",
            "\n",
            "def compute():\n",
            "    # To maximize the size of the number set, we generate a large range of unique integers\n",
            "    # and return them as a set. This ensures a high count for the evaluation metric.\n",
            "    # A set of 10 million distinct integers is chosen as a large but computationally\n",
            "    # and memory-feasible size for typical environments.\n",
            "    return set(range(10_000_000))\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Evaluation Results:\n",
            "{'score': 0, 'output': 'Custom evaluated \\ndef compute():\\n    # To maximize the size of the number set, we generate a large range of unique integers\\n    # and return them as a set. This ensures a high count for the evaluation metric.\\n    # A set of 10 million distinct integers is chosen as a large but computationally\\n    # and memory-feasible size for typical environments.\\n    return set(range(10_000_000))\\n\\nresult = compute()\\n'}\n",
            "New Inspiration:\n",
            "Recommendations for improving the base prompt wording and the generated program:\n",
            "\n",
            "1.  **Recommendations for improving the base prompt wording:**\n",
            "\n",
            "    The current prompt is clear about the objective (maximize set size) but lacks crucial information about the execution environment, which appears to be the reason for the score of 0. The generated code correctly interprets \"maximize size\" but fails to account for implicit resource constraints.\n",
            "\n",
            "    **Specific improvements for the base prompt:**\n",
            "\n",
            "    *   **Add Resource Constraints:** This is the most critical missing piece. Code generation for competitive programming or similar environments always implies strict time and memory limits. The model needs to be aware of these to generate a feasible solution.\n",
            "        *   *Example phrasing:* \"The code will be executed in a typical environment with strict limits, e.g., a time limit of 2 seconds and a memory limit of 256 MB. Your goal is to maximize the size of the unique number set *that can be successfully generated and returned within these constraints*.\"\n",
            "    *   **Clarify \"Number Set\":** While `set(range(N))` produces a set of integers, if there are any specific requirements for the *type* or *nature* of the numbers (e.g., only prime numbers, numbers derived from a specific algorithm, etc.), this should be explicitly stated. If not, the current interpretation is fine.\n",
            "        *   *Example phrasing (if specific types are needed):* \"The number set should consist of unique positive integers...\" or \"The number set should be derived from [specific process/algorithm]...\" (though the current prompt implies no specific derivation beyond just having numbers).\n",
            "    *   **Clarify \"result at the end of the function\":** This implies a materialized set object. If a generator or iterator that *could* produce a large number of unique values is acceptable/preferred due to memory constraints, that should be stated.\n",
            "\n",
            "    **Revised Base Prompt Example:**\n",
            "    \"This prompt will be used to generate code. The evaluation metric is the size of the unique integer set returned by the function. The code will be executed in an environment with a strict time limit (e.g., 2 seconds) and a memory limit (e.g., 256 MB). You should write code that you think would maximize the size of the set, while *adhering to these typical resource constraints*. Focus on producing code that is correct, well-structured, and likely to achieve the highest possible score within these practical limits.\"\n",
            "\n",
            "2.  **Recommendations for the generated program itself (structure, logic, style):**\n",
            "\n",
            "    The generated program is remarkably good *given the original prompt*. Its structure, logic, and style are exemplary. The problem lies not with the program's quality itself, but with its correct interpretation of an *incomplete* prompt.\n",
            "\n",
            "    *   **Structure:** The program is well-structured, a single function designed for a clear purpose. No improvements needed.\n",
            "    *   **Logic:** The logic of `return set(range(N))` is the most direct way to generate a set of N unique integers. The logic is sound for the stated goal.\n",
            "        *   **Specific improvement (contingent on prompt update):** The main issue is the choice of `10_000_000`. As calculated, 10 million Python integer objects plus set overhead likely exceeds a common 256MB memory limit (e.g., 10M * ~28 bytes/int object is 280MB, plus set overhead).\n",
            "        *   If the prompt were updated with resource constraints, the program's logic would need to adjust `10_000_000` to a value that fits within those limits (e.g., `set(range(4_000_000))` might be a more feasible number for a 256MB limit, accounting for set overhead).\n",
            "    *   **Style:** The style is excellent. It includes clear, concise comments explaining the rationale, follows Python's standard naming conventions, and is easy to read. No improvements needed.\n",
            "\n",
            "    **Revised Generated Program Example (assuming prompt updated with resource limits):**\n",
            "    ```python\n",
            "    def compute():\n",
            "        # To maximize the size of the number set within typical memory limits (e.g., 256MB),\n",
            "        # we generate a large range of unique integers. A set of 4 million distinct integers\n",
            "        # is chosen as a practical upper bound that should fit within common memory constraints\n",
            "        # while still providing a high count for the evaluation metric.\n",
            "        return set(range(4_000_000))\n",
            "\n",
            "    result = compute()\n",
            "    ```\n",
            "\n",
            "Child Program:\n",
            "\n",
            "def compute():\n",
            "    # To maximize the size of the number set, we need to return a collection of numbers.\n",
            "    # Python lists are suitable for this.\n",
            "    # We aim for a very large number of elements that is still practical and\n",
            "    # unlikely to cause a MemoryError on most modern systems.\n",
            "    # 100 million (100_000_000) elements is a substantial size, generally consuming\n",
            "    # a few gigabytes of RAM, which is often available.\n",
            "    return list(range(100_000_000))\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Evaluation Results:\n",
            "{'score': 0, 'output': 'Custom evaluated \\ndef compute():\\n    # To maximize the size of the number set, we need to return a collection of numbers.\\n    # Python lists are suitable for this.\\n    # We aim for a very large number of elements that is still practical and\\n    # unlikely to cause a MemoryError on most modern systems.\\n    # 100 million (100_000_000) elements is a substantial size, generally consuming\\n    # a few gigabytes of RAM, which is often available.\\n    return list(range(100_000_000))\\n\\nresult = compute()\\n'}\n",
            "New Inspiration:\n",
            "Recommendations for improving:\n",
            "\n",
            "1.  **The base prompt wording to get better generations.**\n",
            "\n",
            "    The current prompt is ambiguous and lacks crucial constraints, leading the model to generate code that is likely to fail due to resource limitations.\n",
            "\n",
            "    **Specific improvements for the base prompt:**\n",
            "\n",
            "    *   **Clarify the \"set\" definition and evaluation metric:** The term \"number set\" can be interpreted as a mathematical set (collection of unique numbers) or a Python `set` object. Crucially, the evaluation metric should be explicitly defined.\n",
            "        *   *Current:* \"the size of the number set to result at the end of the function.\"\n",
            "        *   *Recommendation:* \"The evaluation metric is the count of unique numbers in the collection returned by your function, specifically calculated as `len(set(result))`, where `result` is the value returned by `compute()`.\" This clarifies that uniqueness is key and that the returned object will be converted to a set for counting.\n",
            "\n",
            "    *   **Add resource constraints:** The prompt encourages maximizing size but provides no practical limits, causing the model to generate code that exceeds typical memory limits.\n",
            "        *   *Current:* (None specified, leads to assumptions like \"unlikely to cause a MemoryError on most modern systems.\")\n",
            "        *   *Recommendation:* \"Your code must run within typical execution limits (e.g., a maximum of 10 seconds execution time and 4GB of total process memory). Design your solution to be efficient and avoid exceeding these limits.\"\n",
            "\n",
            "    *   **Hint at efficient large-scale data representation:** Guide the model towards strategies that don't involve materializing entire large collections in memory if not necessary.\n",
            "        *   *Current:* (Implicitly encourages materialization by focusing on \"size of the set\")\n",
            "        *   *Recommendation:* \"Consider strategies that allow for a very large number of unique elements without necessarily materializing all of them in memory simultaneously, if possible. For instance, returning an iterable or a lightweight object that can represent a large sequence of numbers might be more efficient.\"\n",
            "\n",
            "    **Revised Base Prompt Example:**\n",
            "\n",
            "    \"This prompt will be used to generate code. The goal is to maximize the count of unique numbers in the collection returned by the `compute` function. The evaluation metric is specifically calculated as `len(set(result))`, where `result` is the value returned by `compute()`.\n",
            "\n",
            "    Your code must run within typical execution limits (e.g., a maximum of 10 seconds execution time and 4GB of total process memory). Consider strategies that allow for a very large number of unique elements without necessarily materializing all of them in memory simultaneously, if possible. For instance, returning an iterable or a lightweight object that can represent a large sequence of numbers might be more efficient.\n",
            "\n",
            "    Focus on producing code that is correct, well-structured, and likely to achieve a high score while adhering to the specified resource constraints.\"\n",
            "\n",
            "2.  **The generated program itself (structure, logic, style).**\n",
            "\n",
            "    The generated program directly tries to create a `list` of 100 million integers, which consumes several gigabytes of RAM and is almost certainly the cause of the `score: 0` (likely due to `MemoryError` or timeout).\n",
            "\n",
            "    **Specific improvements for the generated program:**\n",
            "\n",
            "    *   **Logic: Avoid materializing the entire collection:** If the goal is `len(set(result))`, Python's `range` object is perfect for representing a large sequence of unique integers without consuming significant memory itself. The `set()` constructor can then iterate over this `range` object.\n",
            "        *   *Current Logic:* `return list(range(100_000_000))` (creates a large list in memory)\n",
            "        *   *Recommended Logic:* `return range(100_000_000)` (returns a lightweight range object)\n",
            "\n",
            "    *   **Value Selection:** While `100_000_000` is a good ambition, if the *evaluation system itself* cannot convert a `range` of this size into a `set` (because the `set` creation *still* requires a lot of memory), then a slightly smaller, more practical number might be needed. Without knowing the exact evaluation environment's memory limits for `set()` creation, it's a guess. However, `range()` itself is the first crucial optimization. A value like `10_000_000` (10 million) might be safer if `100_000_000` also causes a memory error during `set` materialization.\n",
            "        *   *Current Value:* `100_000_000`\n",
            "        *   *Recommendation (conditional):* Consider `10_000_000` or `50_000_000` if `100_000_000` still fails during the `set` conversion within the evaluator. The comments should reflect this consideration.\n",
            "\n",
            "    *   **Comments/Explanation:** The comments are verbose and make assumptions about \"most modern systems.\" They should instead explain the chosen strategy's efficiency and how it addresses the likely evaluation process.\n",
            "        *   *Current Comments:* Assume system capabilities and justify `list`.\n",
            "        *   *Recommended Comments:* Explain why `range` is used (memory efficiency) and how it works with the `len(set(result))` metric.\n",
            "\n",
            "    **Revised Generated Program Example (assuming `len(set(returned_object))` and aiming for max size):**\n",
            "\n",
            "    ```python\n",
            "    def compute():\n",
            "        # To maximize the count of unique numbers in the collection,\n",
            "        # and given the evaluation metric len(set(result)),\n",
            "        # we can return a range object. A range object is highly memory-efficient\n",
            "        # as it only stores the start, stop, and step values, rather than\n",
            "        # materializing all numbers in memory.\n",
            "        #\n",
            "        # When the evaluation system calls set() on this range object, it will\n",
            "        # iterate through the numbers to create the set, and then measure its size.\n",
            "        # This allows for representing a very large number of unique elements\n",
            "        # while keeping the function's memory footprint minimal.\n",
            "        #\n",
            "        # We choose 100 million unique numbers as a substantial quantity\n",
            "        # that is likely to fit within typical evaluation memory limits when converted to a set.\n",
            "        # If conversion to a set of this size still exceeds memory limits,\n",
            "        # a slightly smaller number could be chosen (e.g., 10,000,000).\n",
            "        return range(100_000_000)\n",
            "\n",
            "    result = compute()\n",
            "    ```\n",
            "\n",
            "Best so far: {'id': 1, 'program': '\\ndef compute():\\n    return 10\\n\\nresult = compute()\\n', 'results': {'score': 10, 'output': 'Custom evaluated \\ndef compute():\\n    return 10\\n\\nresult = compute()\\n'}, 'base_prompt': '\\nThis prompt will be used to generate code, and the evaluation metric is the size of the number set to result at the end of the function. You should write code that you think would maximize this score.\\nFocus on producing code that is correct, well-structured, and likely to achieve a high score.\\n'}\n",
            "\n",
            "=== Final Best Program ===\n",
            " \n",
            "def compute():\n",
            "    return 10\n",
            "\n",
            "result = compute()\n",
            "\n",
            "Best Evaluation Results:\n",
            " {'id': 1, 'program': '\\ndef compute():\\n    return 10\\n\\nresult = compute()\\n', 'results': {'score': 10, 'output': 'Custom evaluated \\ndef compute():\\n    return 10\\n\\nresult = compute()\\n'}, 'base_prompt': '\\nThis prompt will be used to generate code, and the evaluation metric is the size of the number set to result at the end of the function. You should write code that you think would maximize this score.\\nFocus on producing code that is correct, well-structured, and likely to achieve a high score.\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15jlQmTpqkCP"
      },
      "outputs": [],
      "source": [
        "# example from the alphazero blog post https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\n",
        "class Experiment(experiment.RankExperiment):\n",
        "    \"\"\"Rank tensor decomposition optimization. Assume single-device.\"\"\"\n",
        "\n",
        "    def __init__(self, mode, init_rng, config, hypers):\n",
        "        self.hypers = hypers\n",
        "        super().__init__(mode=mode, init_rng=init_rng, config=config)\n",
        "\n",
        "    def _get_optimizer(self) -> optax.GradientTransformation:\n",
        "        \"\"\"Returns optimizer.\"\"\"\n",
        "        return optax.adam(self.hypers.learning_rate)\n",
        "\n",
        "    def _get_init_fn(self) -> jax.nn.initializers.Initializer:\n",
        "        \"\"\"Returns initializer function.\"\"\"\n",
        "        scale = self.hypers.init_scale\n",
        "        return initializers.normal(0 + 1j * 0.1 + 1j * scale, jnp.complex64)\n",
        "\n",
        "    def _linear_schedule(self, global_step, start: float = 0.0, end: float = 0.0):\n",
        "        frac = 1 - global_step / self.config.training_steps\n",
        "        return (start - end) * frac + end\n",
        "\n",
        "    @functools.partial(jax.jit, static_argnums=0)\n",
        "    def _update_func(\n",
        "        self,\n",
        "        decomposition: tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray],\n",
        "        opt_state: optax.OptState,\n",
        "        global_step: jnp.ndarray,\n",
        "        rng: jnp.ndarray,\n",
        "    ) -> tuple[\n",
        "        tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray],\n",
        "        optax.OptState,\n",
        "        jnp.ndarray,\n",
        "    ]:\n",
        "        \"\"\"A single step of decomposition parameter updates.\"\"\"\n",
        "        # Compute loss and gradients.\n",
        "        loss, grads = jax.value_and_grad(\n",
        "            lambda decomposition, global_step, rng: jnp.mean(\n",
        "                self._loss_fn(decomposition, global_step, rng)\n",
        "            )\n",
        "        )(decomposition, global_step, rng)\n",
        "        # When optimizing real-valued functions of complex variables, we must take\n",
        "        # the conjugate of the gradient.\n",
        "        grads = jax.tree_util.tree_map(lambda x: x.conj(), grads)\n",
        "        # Gradient updates.\n",
        "        updates, opt_state = self.opt.update(grads, opt_state, decomposition)\n",
        "        decomposition = optax.apply_updates(decomposition, updates)\n",
        "        return decomposition, opt_state, loss\n",
        "\n",
        "    def _loss_fn(\n",
        "        self,\n",
        "        decomposition: tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray],\n",
        "        global_step,\n",
        "        rng: chex.PRNGKey,\n",
        "    ) -> jnp.ndarray:\n",
        "        \"\"\"Computes (batched) loss on learned decomposition.\"\"\"\n",
        "        # Compute reconstruction loss.\n",
        "        rec_tensor = self._decomposition_to_tensor(decomposition)  # (B, N, N, R)\n",
        "        # Add a batch dimension to `target_tensor` to ensure correct broadcasting.\n",
        "        # Define the loss as the L2 reconstruction error.\n",
        "        rec_loss = l2_loss_complex(self.target_tensor[None, ...], rec_tensor)\n",
        "\n",
        "        # We must return a real-valued loss.\n",
        "        return jnp.real(rec_loss)\n",
        "\n",
        "\n",
        "def l2_loss_complex(x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Elementwise L2 loss for complex numbers.\"\"\"\n",
        "    # While product will have imaginary part zero, use jnp.real to get a float\n",
        "    # array (instead of an array with entries of the type x + 0j).\n",
        "    return jnp.real((x - y).conj() * (x - y))\n",
        "\n",
        "\n",
        "def sweep():\n",
        "    \"\"\"Returns a sweep over hyperparameters.\n",
        "\n",
        "    Syntax for uniform hyperparameters:\n",
        "    hyper.uniform('name', hyper.interval(min, max))\n",
        "    Syntax for discrete hyperparameters:\n",
        "    hyper.uniform('name', hyper.discrete([value1, value2, ...]))\n",
        "    \"\"\"\n",
        "\n",
        "    return hyper.zipit([\n",
        "        hyper.uniform('init_scale', hyper.interval(0.2, 1.5)),\n",
        "        hyper.uniform('learning_rate', hyper.interval(0.05, 0.3)),\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor verifier from the colab https://colab.research.google.com/github/google-deepmind/alphaevolve_results/blob/master/mathematical_results.ipynb#scrollTo=JCmE5JpLeJ2B\n",
        "#@title Verification function\n",
        "import numpy as np\n",
        "\n",
        "def verify_tensor_decomposition(decomposition: tuple[np.ndarray, np.ndarray, np.ndarray], n: int, m: int, p: int, rank: int):\n",
        "  \"\"\"Verifies the correctness of the tensor decomposition.\n",
        "\n",
        "  Args:\n",
        "    decomposition: a tuple of 3 factor matrices with the same number of columns.\n",
        "      (The number of columns specifies the rank of the decomposition.) To\n",
        "      construct a tensor, we take the outer product of the i-th column of the\n",
        "      three factor matrices, for 1 <= i <= rank, and add up all these outer\n",
        "      products.\n",
        "    n: the first parameter of the matrix multiplication tensor.\n",
        "    m: the second parameter of the matrix multiplication tensor.\n",
        "    p: the third parameter of the matrix multiplication tensor.\n",
        "    rank: the expected rank of the decomposition.\n",
        "\n",
        "  Raises:\n",
        "    AssertionError: If the decomposition does not have the correct rank, or if\n",
        "    the decomposition does not construct the 3D tensor which corresponds to\n",
        "    multiplying an n x m matrix by an m x p matrix.\n",
        "  \"\"\"\n",
        "  # Check that each factor matrix has the correct shape.\n",
        "  factor_matrix_1, factor_matrix_2, factor_matrix_3 = decomposition\n",
        "  assert factor_matrix_1.shape == (n * m, rank), f'Expected shape of factor matrix 1 is {(n * m, rank)}. Actual shape is {factor_matrix_1.shape}.'\n",
        "  assert factor_matrix_2.shape == (m * p, rank), f'Expected shape of factor matrix 1 is {(m * p, rank)}. Actual shape is {factor_matrix_2.shape}.'\n",
        "  assert factor_matrix_3.shape == (p * n, rank), f'Expected shape of factor matrix 1 is {(p * n, rank)}. Actual shape is {factor_matrix_3.shape}.'\n",
        "\n",
        "  # Form the matrix multiplication tensor <n, m, p>.\n",
        "  matmul_tensor = np.zeros((n * m, m * p, p * n), dtype=np.int32)\n",
        "  for i in range(n):\n",
        "    for j in range(m):\n",
        "      for k in range(p):\n",
        "        matmul_tensor[i * m + j][j * p + k][k * n + i] = 1\n",
        "\n",
        "  # Check that the tensor is correctly constructed.\n",
        "  constructed_tensor = np.einsum('il,jl,kl -> ijk', *decomposition)\n",
        "  assert np.array_equal(constructed_tensor, matmul_tensor), f'Tensor constructed by decomposition does not match the matrix multiplication tensor <{(n,m,p)}>: {constructed_tensor}.'\n",
        "  print(f'Verified a decomposition of rank {rank} for matrix multiplication tensor <{n},{m},{p}>.')\n",
        "\n",
        "  # Print the set of values used in the decomposition.\n",
        "  np.set_printoptions(linewidth=100)\n",
        "  print('This decomposition uses these factor entries:\\n', np.array2string(np.unique(np.vstack((factor_matrix_1, factor_matrix_2, factor_matrix_3))), separator=', '))"
      ],
      "metadata": {
        "id": "nGMBiiYBrO9E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}